% CREATED BY DAVID FRISK, 2016
\begin{thebibliography}{69}

\bibitem{Reference:ref1} Frisk, D. (2016) A Chalmers University of Technology Master's thesis template for \LaTeX . Unpublished.


\bibitem{article1} Deep Learning with Limited Numerical Precision, "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan"
\bibitem{article2} Mixed-precision training of deep neural networks using computational memory , Nandakumar S. R., Manuel Le Gallo, Irem Boybat,Bipin Rajendran, Abu Sebastian,Evangelos Eleftheriou1
\bibitem{article3} Mixed-precision architecture based on computational memory for training deep neural networks, S. R. Nandakumar, Manuel Le Gallo, Irem Boybat, Bipin Rajendran†, Abu Sebastian and Evangelos Eleftherin
\bibitem{article3} Scaling Neural Machine Translation, Myle Ott, Sergey Edunov, David Grangier, Michael Auli
* INTRODUCTION TO MIXED PRECISION TRAINING, Dusan Stosic, NVIDIA
\bibitem{article4} MIXED PRECISION TRAINING, Sharan Narang, Gregory Diamos, Erich Elseny, Paulius Micikevicius, Jonah Alben, David Garcia, Boris Ginsburg, Michael Houston,Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu

\bibitem{article5}  Deep Learning Inference Using Intel® FPGAs,  Philip Colangelo, Nasibeh Nasiri, Asit Mishra, Eriko Nurvitadhi, Martin Margala, Kevin Nealis
\bibitem{article6} Efficient Fixed/Floating-Point Merged Mixed-Precision Multiply-Accumulate Unit for Deep Learning Processors, Hao Zhang , Hyuk Jae Lee  and Seok-Bum Ko 
\bibitem{article7} Rethinking floating point for deep learning, Jeff Johnson Facebook AI Research
\bibitem{article8} DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING , Song Han, Huizi Mao ,William J. Dally
\bibitem{article9} DEEP LEARNING PERFORMANCE , User Guide, NVIDIA


\bibitem{article10} Timeloop: A Systematic Approach to DNN Accelerator Evaluation, Angshuman Parashar, Priyanka Raina, Yakun Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan Venkatesan, Brucek Khailany ,Stephen W. Keckler, Joel Emer
\bibitem{article11} The Deep Learning Revolution and Its for Comput er Architecture and Chip Design, Jeffrey Dean , Google Research
\bibitem{article12} Harnessing Numerical Flexibility for Deep Learning on FPGAs, Andrew C. Ling , Mohamed S. Abdelfattah, Andrew Bitar,David Han, Roberto Dicecco, Suchit Subhaschandra, Chris N Johnson, Dmitry Denisenko, Josh Fender, Gordon R. Chiu
 \bibitem{article13} MIXED PRECISION TRAINING OF DEEP NEURAL NETWORKS ,Carl Case, NVIDIA
\bibitem{article14} MLPERF INFERENCE BENCHMARK, Vijay Janapa Reddi , Christine Cheng , David Kanter ,Peter Mattson , Guenther Schmuelling , Carole-JeanWu , Brian Anderson , Maximilien Breughe , Mark Charlebois , William Chou , Ramesh   hukka , Cody Coleman , Sam Davis , Pan Deng , Greg Diamos ,Jared Duke , Dave Fick , J. Scott Gardner , Itay Hubara , Sachin Idgunji , Thomas B. Jablin , Jeff Jiao , Tom St. John , Pankaj Kanwar , David Lee ,  effery Liao , Anton Lokhmotov ,Francisco Massa , Peng Meng , Paulius Micikevicius , Colin Osborne , Gennady Pekhimenko , Arun Tejusve Raghunath Rajan ,Dilip Sequeira , Ashish Sirasao , Fei Sun ,Hanlin Tang  ,Michael Thomson , Frank Wei , EphremWu , Lingjie Xu , Koichi Yamada , Bing Yu ,George Yuan , Aaron Zhong , Peizhao Zhang , Yuchen Zhou 
\bibitem{article15} Stream Semantic Registers: A Lightweight RISC-V ISA Extension Achieving Full Compute Utilization in Single-Issue Cores , Fabian Schuiki, Florian Zaruba, Torsten Hoefler, and Luca Benini
\bibitem{article16} A Domain-Specific Architecture for Deep Neural Networks , NORMAN P. JOUPPI, CLIFF YOUNG, NISHANT PATIL, AND DAVID PATTERSON
\bibitem{article17} MIXED PRECISION TRAINING OF CONVOLUTIONAL NEURAL NETWORKS USING INTEGER OPERATIONS, Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, Alexander Heinecke, Pradeep Dubey, Nikita Shustrov, Roma Dubtsov, Evarist Fomenko, Vadim Pirogov, Jesus Corbal
\bibitem{article18} Fine-Grained Exploitation of Mixed Precision for Faster CNN Training
\bibitem{article19} Yongming Shen, Michael Ferdman, and Peter Milder. Maximizing CNN  accelerator  efficiency through resource partitioning.
\bibitem{article20} Yufei Ma, Yu Cao, Sarma Vrudhula, and Jae-sun Seo. Optimizing loop operation and ataflow in FPGA acceleration of deep convolutional neural networks.
\bibitem{article21} Mohammad Motamedi, Philipp Gysel, Venkatesh Akella, and Soheil Ghiasi. Design space exploration of FPGA-based deep convolutional  neural networks.  
\bibitem{article22} Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna. Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects.
\bibitem{article23} Dally, W. High-performance hardware for machine learning. Invited talk at Cadence ENN Summit (Santa Clara, CA, Feb. 9, 2016)
\bibitem{article24} Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights  and connections for efficient neural networks. In Proceedings of Advances in Neural Information Processing Systems (Montreal Canada, Dec.) MIT Press, Cambridge, MA, 2015
\bibitem{article25} Chen, Y., Chen, T., Xu, Z., Sun, N., and Teman, O. DianNao Family: Energy-efficient hardware accelerators for machine learning. Commun. ACM 59, 11 (Nov. 2016), 105–112. 8. Chen, Y.H., Emer, J., and Sze, V. Eyeriss: A spatial architecture for energy-efficient dataflow for  convolutional neural networks. In Proceedings of the 43rd ACM/IEEE International Symposium on Computer Architecture (Seoul, Korea), IEEE Press, 2016.
\bibitem{article26} Ienne, P., Cornu, T., and Kuhn, G. Special-purpose digital hardware for neural networks: An architectural survey. Journal of VLSI Signal Processing Systems for Signal, Image and Video Technology 13, 1 (1996), 5–25.
\bibitem{article27} Atul Rahman, Sangyun Oh, Jongeun Lee, and Kiyoung Choi. Design space exploration of FPGA accelerators for convolutional neural networks.  





\end{thebibliography}
