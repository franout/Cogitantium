% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}
Machine learning is one of the hot technologies today as it is being used to solve complex problems that would otherwise be very hard or costly to solve with traditional methods. Speech and image recognition, as well as many other complex decision-making problems such as self-driving vehicles are successfully solved with machine learning and deep-learning. 
The success of machine learning is being driven by the current available hardware which can provide the required demands in terms of storage and compute capacity. But obviously as problems scale so do the demands and thus special hardware is being developed to address these needs. \\\\
The use of commodity hardware is not the most effective and efficient way to address this problem, so research is looking at solution that can satisfy the required demands but at lower cost and energy consumption in order to support machine learning also on mobile devices.
The constant developments in the machine learning methods require the design of more flexible hardware accelerators \cite{paper:1} \cite{paper:2}.\\
As it is very well-known, hardware accelerators are capable, if designed correctly, of delivering a lot of improvements in terms of the latency but also in terms of energy efficiency\cite{paper:29}. Thus, in order to obtain the best solution in every metric a hardware-software co-design is needed, requiring to the designer a basic knowledge of machine learning algorithms.\\\\

In the last years, the number of published papers regarding Machine Learning has growth exponentially, leading to significant efforts also from the companies which started to develop, deploy and sell their own hardware platforms, such as Tensor Processing Unit \cite{paper:40} from Google, NVDLA\cite{WEBSITE:6} from Nvidia and Gaudi \cite{paper:39} and Goya\cite{paper:38}, respectively for training and interference, from Habana (acquired by Intel).\\\\
Machine Learning includes two processes, the training and the inference. Mainly, the work is focused on the inference process, exploring different optimizations in terms of hardware.\\\\The goal is to develop an hardware accelerator for the inference process of Machine Learning applications, which is also configurable in terms of arithmetic precision for accomodating different precision requests from different models. Most of the computations in the inference process are composed of multiply and accumulate of data, where some of them are often reused, the accelerator will speed-up those aspects, exploiting a non Von-Neumann architecture. It is designed from scratch, therefore the work is carried out on FPGA in order to have the possibility of exploring different solutions and measuring different design metrics of the prototype. Integrating the hardware with an high level ML-FrameWork will also allow to meausure the overall latency of the Neural Network model for real time application and compare performances with different accelerator size for finding the best trade-off for a mobile device.\\\\
One of the main ideas of the work is that during the inference process, a model does not need high precision computations \cite{paper:8} \cite{paper:15}for achieving high accuracy into its outputs. Thus, the use of different arithmetic data type can drastically reduce the computations without reducing the final accuracy of the DNN \cite{paper:7} \cite{paper:8}. From a hardware perspective,  the use of different arithmetic precision\cite{paper:14}, such as the use of integer operations instead of floating point operations, can lead to benefits in terms of area, energy consumption and latency. Following the benefits previosly mentioned, the final system may be deployed on energy-area constrained devices. Moreover, the possibility of decide the current arithmetic precision of the accelerator allows the user, or the high level application, to use different neural network models leading to the use of different  applications and/or domains of the same hardware, or even at the same time from the user perspective, actually the two models could be multiplexed in time on the accelerator.\newline
According to that, accuracy of operations, reliability, performance and energy efficiency are evaluated, moving through different designs with different precision and compared to the implementation of the same DNN inside a GPU, as well as custom hardware platform.


Please organize better your thoughts. You can not have a goal and then a "main idea"... specify clearly what do you want. TO build an accelerator. Then that it is energy efficient. What do you do for that? You use FPGA as a technology for easy reconfiguration and flexibility and also you explore using limited precision for more energy efficiency!
Also please state clearly a "research question" that you plan to answer with your work!