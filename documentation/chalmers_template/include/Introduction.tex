% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}
Machine Learning is one of the hot technologies today as it is being used to solve complex problems that would otherwise be very hard or costly to solve with traditional methods. Speech and image recognition, as well as many other complex decision-making problems such as self-driving vehicles are successfully solved with Machine Learning and Deep-Learning. \\
The success of Machine Learning is being driven by the current available hardware which can provide the required demands in terms of storage and compute capacity. But obviously as problems scale so do the demands and thus special hardware is being developed to address these needs. \\\\
The use of commodity hardware is not the most effective and efficient way to address this problem, so research is looking at solution that can satisfy the required demands but at lower cost and energy consumption in order to support Machine Learning also on mobile devices.
The constant developments in the Machine Learning methods require the design of more flexible hardware accelerators \cite{paper:1} \cite{paper:2}.\\
As it is very well-known, the hardware accelerators are capable, if designed correctly, of delivery a lot of improvements in terms of the latency but also in terms of energy efficiency\cite{paper:29}. Thus, in order to obtain the best solution in every metric a hardware-software co-design is needed, requiring a basic knowledge of Machine Learning algorithms.\\\\

In the last years, the number of published papers regarding Machine Learning has growth exponentially, leading to a lot of efforts also from the companies which started to develop, deploy and sell their own hardware platforms, such as Tensor Processing Unit \cite{paper:40} from Google, NVDLA\cite{WEBSITE:6} from Nvidia and Gaudi \cite{paper:39} and Goya\cite{paper:38}, respectively for training and interference, from Habana (acquired by Intel).\\\\
Machine Learning includes two processes, the training and the inference. Mainly, the work is focused on the inference process, exploring different optimizations in terms of hardware.\\\\The aim of the work is to develop an flexible, especially in terms of arithmetic precision, hardware accelerator for Machine Learning applications, since it is a design from scratch the work is carried out on FPGA in order to have the possibility of exploring different solutions and measuring different design metrics of the prototype.\\\\
One of the main ideas of those works is that during the inference process, a model does not need the high precision computations \cite{paper:8} \cite{paper:15}for achieving high accuracy into its outputs. Thus, the use of different arithmetic data type can drastically reduce the computations without reducing the final accuracy of the DNN \cite{paper:7} \cite{paper:8}. From a hardware perspective,  the use of different arithmetic precision\cite{paper:14} can lead to benefits in terms of area, energy consumption and latency. Moreover, the possibility of decide the current arithmetic precision of the accelerator allows the user, or the high level application, to use different Neural Network models leading to the use of different  applications and/or domains of the same hardware, or even at the same time from the user perspective, actually the two models could be multiplexed in time on the accelerator.\newline
According to that, accuracy of operations, reliability, performance and energy efficiency are evaluated, moving through different designs with different precision and/or data gating and compared to the implementation of the same DNN inside a GPU, as well as custom hardware platform.


