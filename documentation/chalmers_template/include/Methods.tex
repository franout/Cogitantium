% CREATED BY DAVID FRISK, 2016
\chapter{System Development}

\section{Overview}
As already mentioned, the use of custom hardware for a specific application can have big benefits especially in terms of energy consumption and latency.
The inference process of Neural Network is mainly characterized by massive multiply and addition operations. Fetch of data from main memory follows patterns and it has been proved those data, in particular weight data, are reused for several exection of the Neural Network model.
As consequence, executing a Neural Network model on a Von-Neumann based architecture machine leads to perfomance degradation, even in a cache-based system, since the CPU has to request the data from the main memory, execute the operation on those data and then save back to main memory before moving to the next data. The introduction of vectored instrucion in the modern processors can have a slight impact in the performance benefits. However, the drastically increase of layers in the Neural Network has made them suitable for several applications, this it can be translated into a massive increase of operations for executing them. Following the fast demands of operations into a Neural Networks, it becomes evident that executing them on a CPU could not meet anymore real-time application requirements.\\
Instead, the approach of the accelerator is a Dataflow architecture, with emphasis on weight data reuse. The basic idea is a computation matrix composed in every entry of processing elements which are able to perform operation between the incoming data and the weights, which have been already loaded for exploiting a data reuse approach.\\\\
After a preliminary research on which ML-Framework would allow to integrate a custom hardware accelerator minimizing the efforts to change the model code and its definitions, it has been evident that the TensorFlow Framework suits the needs.\\
\newpage
The workflow of the Hardware-Software development is illustrated in the following:
\todo{t seems complete but looks complicated... Can all the inputs be on the same side? On the left? And then can we have a result output on the right? BTW no output so far?
Can you also use colors to make it easier to read so that different functionalities for example are grouped in different c}
\begin{figure}[!htbp]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.5]{./figure/workflow.png}
\caption{Development workflow}
\label{fig:workflow}
\end{figure}

\newpage
The entire work is implemented on a PYNQ Z2 board from TUL, based on a Zynq-7000 SoC \cite{paper:31}. In order to speed-up the development process and use built-in library for the AXI protocol and the DMA transfers, the software is carried out through the PYNQ enviroment of the board \cite{WEBSITE:2} based on Python which has become a de facto standard \cite{paper:37}. \\
The usage of Python as basic software allows to easily integrate it with high level Machine Learning FrameWork, such as TensorFlow in this case, an end-to-end open source Machine Learning platform \cite{WEBSITE:4}. 
\newpage
\section{Software}

The aim of the work is focused on the inference process, pre-trained models are needed and TensorFlow Hub \cite{WEBSITE:5} comes in handy for this purpose. It provides already pre-trained Machine Learning models for different domains. Moreover, TensorFlow has the feature of quantize a post-trained model for different arithmetic precision.\\

In the Fig. \ref{fig:workflow} it can be seen that the quantization prosses has been done offline. It is worth to mention that TensorFlow is open-source and since no binary installations for its 2.1 release are provided for Arm processor, it has been cross-compiled from scratch for the PYNQ-Z2 board.\\

The choice of using the stable release 2.1 of TensorFlow is dictated from the possibility of using Delegates (aka hardware accelerators or GPUs) in its Neural Network model. A delegate is a way to delegate part or all graph exection to another executor. Every model is represented, internally, as a graph (with its relative order of exection for the nodes) and every node of the graph is described as a set of operation that has to be applied to the node's input. As every nodes is described by a set of operations, it is easy to understand which part of the graph can be executed on the accelerator in advance, and this operation is done at the beginning when both the model and the accelerator library is loaded.\\\\

TensorFlow demands as a library for the accelerator a C Python-API compatible shared library. In addition, the code for using the accelerator was already written using the PYNQ enviroment in Python. Therefore, for allowing code reuse and  decreasing the development time the Python code has been embedded in the C code (from a Tensoflow example of the delegate library), adding callbacks to Python code\footnote{See Appendix 1}. This has been possible thanks to the Python library \textit{CFFI} (C Foreign Fucntion Interface)\cite{WEBSITE:14}, which is also able to provide a shared library Python-API compatible.


\newpage
\section{System Level}

As it can be seen from \ref{fig:zynq} , it is divided in two big block:
\begin{itemize}
\item Processing System:
The processing system in Figure \ref{fig:sys} referred as \textit{ps7} is in charge of running the OS and the Machine Learning application, as consequence it also runs the necessary software for programming the accelerator registers and the data movement to/from main memory from/to the accelerator.
\item Programmable Logic:
The programmable logic (PL) hosts the entire design, from the accelerator itself to the DMAs and the AXI interconnections.\\ Several single channel DMA have been used instead of using a single DMA with multiple channels, the reason is that the in the PYNQ enviroment only the drivers for the single channel DMA are provided. Furthermore, the Programmable Logic can be divided into:
\begin{itemize}
\item AXI interconnections: IP cores from Xilinx\cite{paper:34}\cite{paper:35} in order to connect and correctly address entities in the Programmable Logic.
\item AXI DMA: IP core from Xilinx \cite{paper:33} which allows data movement between main memory and accelerator memories.
\item DTPU: the actual hardware accelerator.
\item XADC: IP core from Xilinx \cite{paper:32} which allows to measure the temperature of the SoC, the voltages and the currents at runtime.

\end{itemize}

\end{itemize}

\begin{figure}[!htbp]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.35]{./figure/zynq.PNG}
\caption{Zynq 7000 SoC\cite{paper:42}}
\label{fig:zynq}
\end{figure}


\newpage
In the following figure, the schematic of the overall design in the PL is presented.
\begin{figure}[!htbp]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=1,angle=90]{./figure/system_schematic.pdf}
\caption{System}
\label{fig:sys}
\end{figure}

\section{DTPU, the hardware accelerator}
The hardware accelerator, named \textit{ Dumb Tensor Processing Unit}, is in charge of carring out the computation of the neural network model, exploiting a data-flow architecture on the input data applyng a Fused Multiply Add approach between each unit of the Matrix Multiplication Unit (MXU).

In Figure \ref{fig:logaccel} is presented the Logical block diagram of the accelerator.
\begin{figure}[!htbp]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.40]{./figure/logical_view.png}
\caption{Logical view of the accelerator}
\label{fig:logaccel}
\end{figure}
\subsection{Real Implementation}
The work is not focused on developing embedded memories and AXI interfaces, therefore a Xilinx's IP core, which includes all the necessary subcomponents, has been used\cite{paper:43} leading to the actual block diagram which can be observed in the Figure \ref{fig:rtlaccel}.
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=1.1,angle=90]{./figure/accelerator_schematic.pdf}
\caption{Real RTL view of the accelerator}
\label{fig:rtlaccel}
\end{figure} 
The latter has allowed to completely focus the work on the DTPU core.
\newpage
\subsection{DTPU core}
As it is well known, the execution of ML models is memory intensive and it consists in massive multiplication and accumulation operations. In addition, it can be seen also that during execution of ML models some memory location are accessed frequentely. Therefore, it is evident that a DataFlow architecture which could exploit local data reuse and compute, massively, in parallel multiplications and additions  could boost the performance.
The DTPU core has been designed according to the previously mentioned ideas.
The core is presented in Figure \ref{fig:datapath} as block diagram.
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.3,angle=0]{./figure/datapath_dtpu_core.png}
\caption{A detailed view of the DTPU core datapath. Enable and Resets signals for clocked units has been omitted for improving readability. }
\label{fig:datapath}
\end{figure} 

In Fig. \ref{fig:datapath}, the brawn of the accelerator is the MXU wrapper, which contains the matrix (symmetric) of MACs with variable precision. Regarding the other blocks:
\begin{itemize}
\item Activation Decoder: It is able to generate the right activation signals for the L/S units, depending from the current data precision and MXU size.
\item Muxes and DeMuxes: Their purpose is to feed the right data from/to memory to/from the right units. The counter (from 0 to ROWS-1) in the Mux for the output fifo is for saving at every clock cycle a data in the FIFO.
\item Filter\&Select: depending on the precision it provides the correct data to the correct computation units.
\item Compact\&Select: it is the complement of the Filter\&Select unit, it is able to compact the output data from the MXU wrapper and feed the store registers.
\item L/S weight Units: the name L/S has been kept for consistency even if it does not have any store process since the weight are only loaded once (stationary weights) and kept until a next full execution. 
\item L/S Activation Units:  they are in charge of loading the data from the input FIFO into batteries of Flip-Flops while at the same time they can save the results to submit late in the output fifo.
\end{itemize}

\subsubsection{High Level State Machine of Control Unit}
\todo[inline]{state diagram of cu}


\subsubsection{Filter\&Select and Compact\&Select}
In principle, for each Processing element in the MXU wrapper a weight and an activation has to be provided (and as consequence it has to be provided from its relative Load Units). However, since the datawidth of memories and FIFO has been fixed to its maximum, 64 bits, it comes evident that during a computation with 8 bit integer it will fetch(and save for the output FIFO), in case of a 8x8 Mxu Size, 8 values from FIFOs and 64 values from the weight memory. Moreover, in this scenario all the Flip-Flops of the L/S units (both activation and weights) would sample values where the 56 upper bits are always unused leading to a waste of time for the memory accesses and energy for unused data movement.\\ A clever solution is to pack data, increasing the memory bandwith and reducing the active L/S units (reducing also the energy consumption). Nevertheless, the pack of data requires to internally unpack and, before committing to the output fifo, pack the results.
Unpacking and packing are done, respectively, by Filter\&Select and Compact\&Select units.
Retrieving the previous example (computation on 8 bit integer, MXU size of 8x8 and 64 bit memory data) and using the approach of unpacking and packing, this leads to  use only one L/S unit for activations (8 for the L/S weigth units) for both the load and store operation. With one single L/S active unit and 8 bit integer computation, an  activation data on 8 bit has to be distributed for each column of the MXU, and this is done by the Filter\&Select unit. For commiting to output FIFO, results on 8 bit will be compacted in one single data by the Compact\&Select.\\

In case the required precision is on 16 bit, with the same MXU size, two L/S units for activation are activated (2*ROWS for the L/S weight units) and will feed the respective Columns. The reason behind the two active L/S units is that in 64 bit, only 4 16-bit values can be packed.
Increasing the MXU size, the L/S units are activate accordingly. For example, in case of a MXU size of 16x16 and integer 8 bit, two L/S units are activated ( in case of integer 16 computation,4 units are activated).

The activation of the correct L/S unit depending on the current precision is granted from the Activation Decoder in Fig. \ref{fig:datapath}.
This approach comes also with the overhead of packing and unpacking the data on the CPU but, on the other hand, the memory data movement are reduced and bandwith increased, with a reduction in the energy consumption.

A visual distribution of the data can be seen in Fig. \ref{fig:fscs}, the same can be applied for each row of L/S Weigth units.
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.35,angle=0]{./figure/filter_and_select.png}
\caption{Data Distribution of Filter\&Select unit for a MXU size of 8x8
}
  \small Additional zero input value to Muxes not shown
\label{fig:fscs}
\end{figure} 

It is also worth to mention that using size for the MXU which are power of two would maximize the memory bandwith.

\newpage
\subsubsection{Matrix Multiplication Unit}
The Matrix Multiplication Units (referred as MXU) is the muscle part of the accelerator.
As the name suggest, it is organized as a Matrix:
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.35,angle=0]{./figure/mxu.png}
\caption{MXU interal structure}
\label{fig:mxu}
\end{figure} 

Where every sub units has its own weight value ( distributed thanks to the L/S weight combined with Filter\&Select units , see Fig. \ref{fig:datapath}). It is an homogeneous unit, except for the first column, which does not accumulate. In addition, as it can be seen from the block diagram, there is no control flow between every Processing units which allows a simple Control Unit, there is only data exchange from the previous unit to the next one (for both axis). \\\\
The MXU can be synthesized with different criteria.\\ In particular, the Processing Elements can be independetly generated for a single data precision, from integer 8/16/32/64 to floating point 32 or brain floating point 16, or with several precision at the same time. Then data precision is decided and properly controlled using signal in \ref{fig:smac}.
\newpage
A detailed view of SMAC (Subunit Multiply and Accumulate) and SMUL(Subunit Multiply), the Processing Elements, is given in \ref{fig:smac}.
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.45,angle=0]{./figure/smac.png}
\caption{SMAC and SMUL details }
\label{fig:smac}
\end{figure} 

It is important to mention that the subunits are always receiving data on 64 bits even if internally they may use all of them or not, depending on the value of \textit{select p recision}  and \textit{active chain} signals.
Moreover, for the full integer configuration ( 64 bitwidth operations) beside the possibility of computing for different data width (i.e. choose between 8/16/32/64) with the help of \textit{active chain} signal (active low, otherwise it is a 64 bit computation) and data width fixed to 64 bit, the Processing Elements can compute vectorized operations. Therefore, it is able to compute at the same time two 8bit, one 16bit and one 32bit operations (multiplication for SMUL and multiplication and addition for SMAC).
However, this comes with the overhead of correctly packing and unpacking the data on the CPU before transfering them to the accelerator.
\newpage
SMAC and SMUL units have been designed, internally, using Vivado DSP primitives \cite{paper:48}, which a general schema can be appreciate in Fig. \ref{fig:dsp}:
\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.5,angle=0]{./figure/vivado_dsp.png}
\caption{ DSP Slice Functionality\cite{paper:48} }
\label{fig:dsp}
\end{figure} 
Allowing to fit two computation (referring to SMAC) in one single unit.\\

As soon as the Synthesis process reach the maximum value of DSP utilization, it does not switch automatically to use Fabric for those primitives. For maximizing the resource usage of the FPGA, the DSP primitives have been pre-generated for both Fabric and DSP blocks. In this way, during the generation algorithm for the MXU, it uses primitives for DSP up to the maximum allowed value for the given board and then it starts to utilize Fabric.
This apporach has allowed almost a full utilization of the FPGA resources.

